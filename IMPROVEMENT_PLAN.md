# ğŸ“‹ í•€ë‹¤ CX ìƒë‹´ í‰ê°€ ì‹œìŠ¤í…œ ê°œì„  ê¸°íšì„œ

## ğŸ“Œ Executive Summary

### í”„ë¡œì íŠ¸ëª…
Multi-LLM ê¸°ë°˜ ìƒë‹´ í‰ê°€ ì¼ê´€ì„± ê°•í™” í”„ë¡œì íŠ¸

### ëª©ì 
- í‰ê°€ ì¼ê´€ì„±ì„ 85%ì—ì„œ 95%ë¡œ í–¥ìƒ
- Multi-LLM êµì°¨ ê²€ì¦ì„ í†µí•œ í‰ê°€ ì‹ ë¢°ë„ í™•ë³´
- ì„¤ì • ê¸°ë°˜ ê´€ë¦¬ë¡œ ìš´ì˜ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

### ê¸°ê°„
2024ë…„ 1ì›” 15ì¼ ~ 2024ë…„ 2ì›” 15ì¼ (4ì£¼)

### ì˜ˆìƒ íš¨ê³¼
- í‰ê°€ í¸ì°¨ 60% ê°ì†Œ
- ìš´ì˜ ë¹„ìš© 30% ì ˆê°
- í‰ê°€ ì‹ ë¢°ë„ 40% í–¥ìƒ

---

## ğŸ” 1. AS-IS í˜„í™© ë¶„ì„

### 1.1 ì‹œìŠ¤í…œ êµ¬ì¡°

#### í˜„ì¬ ì•„í‚¤í…ì²˜
```
[Excel Upload] â†’ [Next.js API] â†’ [OpenAI GPT-4] â†’ [í‰ê°€ ê²°ê³¼]
```

#### í•µì‹¬ íŒŒì¼ êµ¬ì¡°
```
app/api/
â”œâ”€â”€ analyze-individual/route.ts    # ê°œë³„ í‰ê°€ (temperature: 0.3)
â”œâ”€â”€ analyze-comprehensive/route.ts # ì¢…í•© í‰ê°€ (temperature: 0.1)
â”œâ”€â”€ analyze-individual-new/route.ts # ì‹ ê·œ í‰ê°€ (temperature: 0.3)
â””â”€â”€ analyze-counselor-comprehensive/route.ts # ìƒë‹´ì› ì¢…í•©
```

### 1.2 í˜„ì¬ í‰ê°€ í”„ë¡œì„¸ìŠ¤

#### ë‹¨ì¼ LLM í‰ê°€
```typescript
// í˜„ì¬ ì½”ë“œ (analyze-individual/route.ts)
async function callOpenAI(prompt: string, apiKey: string) {
  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "chatgpt-4o-latest",
      messages: [
        {
          role: "system",
          content: "ë‹¹ì‹ ì€ í•€ë‹¤ CXíŒ€ì˜ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
        },
        {
          role: "user",
          content: prompt,
        },
      ],
      temperature: 0.3,  // âš ï¸ ì¼ê´€ì„± ë¬¸ì œ
      max_tokens: 1500,
    }),
  })
  // ì—ëŸ¬ ì²˜ë¦¬ ë¯¸í¡
  if (!response.ok) {
    throw new Error(`OpenAI API ì˜¤ë¥˜: ${response.status}`)
  }
  return response.json()
}
```

### 1.3 ì£¼ìš” ë¬¸ì œì 

| ì˜ì—­ | ë¬¸ì œì  | ì˜í–¥ë„ | ì‹¬ê°ë„ |
|------|--------|--------|--------|
| **ì¼ê´€ì„±** | Temperature 0.3ìœ¼ë¡œ ë³€ë™ì„± ì¡´ì¬ | ë†’ìŒ | ğŸ”´ Critical |
| **ë‹¨ì¼ ì˜ì¡´ì„±** | OpenAI APIë§Œ ì‚¬ìš© | ë†’ìŒ | ğŸ”´ Critical |
| **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬** | í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ | ì¤‘ê°„ | ğŸŸ¡ Major |
| **ì—ëŸ¬ ì²˜ë¦¬** | ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¬ | ì¤‘ê°„ | ğŸŸ¡ Major |
| **ë¹„ìš© ê´€ë¦¬** | ë¹„ìš© ì¶”ì  ì‹œìŠ¤í…œ ì—†ìŒ | ë‚®ìŒ | ğŸŸ¢ Minor |
| **ê²€ì¦ ì²´ê³„** | í‰ê°€ ê²°ê³¼ ê²€ì¦ ë¶ˆê°€ | ë†’ìŒ | ğŸ”´ Critical |

### 1.4 í˜„ì¬ í‰ê°€ ê¸°ì¤€

```javascript
// í•˜ë“œì½”ë”©ëœ í‰ê°€ ê¸°ì¤€ (page.tsx)
const DEFAULT_GUIDELINES = `
âš ï¸ CRITICAL RULES (ì ˆëŒ€ ì¤€ìˆ˜)
1. í‰ê°€ ëŒ€ìƒ: ì˜¤ì§ 'ìƒë‹´ì›'ì˜ ë©”ì‹œì§€ë§Œ í‰ê°€
2. ìë™ ë©”ì‹œì§€ ì™„ì „ ì œì™¸
3. ì„œí¬íŠ¸ë´‡ ë©”ì‹œì§€ ì œì™¸
4. ìƒí™© ê³µê° = ìƒí™© íŒŒì•… + í•´ê²° ë°©í–¥ ì œì‹œ
5. ë¬¸ì œ íŒŒì•… ì ê·¹ ë…¸ë ¥ = í•„ìˆ˜ ê°€ì  ìš”ì†Œ
6. ì ìˆ˜ ë‹¤ì–‘ì„± í™•ë³´ (1.0-5.0 ì „ ë²”ìœ„ í™œìš©)
`
```

---

## ğŸ¯ 2. TO-BE ëª©í‘œ ìƒíƒœ

### 2.1 ê°œì„ ëœ ì‹œìŠ¤í…œ êµ¬ì¡°

#### ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜
```
[Excel Upload] â†’ [Next.js API] â†’ [Multi-LLM Orchestrator]
                                          â†“
                              [OpenAI] [Gemini] [Claude*]
                                          â†“
                                  [Cross Validator]
                                          â†“
                                    [í‰ê°€ ê²°ê³¼]
```

### 2.2 Multi-LLM í‰ê°€ ì²´ê³„

#### LLM Provider ì¶”ìƒí™”
```typescript
// lib/llm/providers/base.ts
export interface LLMProvider {
  name: string;
  model: string;
  evaluate(prompt: string): Promise<EvaluationResult>;
  getCost(tokens: number): number;
}

// lib/llm/providers/openai.ts
export class OpenAIProvider implements LLMProvider {
  name = 'openai';
  model = process.env.OPENAI_MODEL || 'gpt-4-turbo-preview';
  
  async evaluate(prompt: string): Promise<EvaluationResult> {
    return withRetry(async () => {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: this.model,
          temperature: 0.1,  // âœ… ì¼ê´€ì„± ê°•í™”
          seed: 42,          // âœ… ì¬í˜„ ê°€ëŠ¥ì„±
          messages: [
            { role: 'system', content: this.getSystemPrompt() },
            { role: 'user', content: prompt }
          ],
          response_format: { type: "json_object" }
        })
      });
      
      if (!response.ok) {
        throw new APIError(`OpenAI API error: ${response.status}`);
      }
      
      return this.parseResponse(await response.json());
    }, 3); // 3íšŒ ì¬ì‹œë„
  }
  
  getCost(tokens: number): number {
    const rates = {
      'gpt-4-turbo-preview': 0.01,
      'gpt-3.5-turbo': 0.001
    };
    return (tokens / 1000) * rates[this.model];
  }
}
```

### 2.3 êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ

#### Cross Validator êµ¬í˜„
```typescript
// lib/evaluation/cross-validator.ts
export class CrossValidator {
  private providers: LLMProvider[];
  
  constructor() {
    this.providers = [
      new OpenAIProvider(),
      new GeminiProvider(),
      // new ClaudeProvider() // ì˜µì…˜
    ];
  }
  
  async evaluate(chat: Chat): Promise<ConsolidatedResult> {
    // ë³‘ë ¬ í‰ê°€
    const evaluations = await Promise.allSettled(
      this.providers.map(provider => 
        this.evaluateWithProvider(chat, provider)
      )
    );
    
    // ì„±ê³µí•œ í‰ê°€ë§Œ í•„í„°ë§
    const successful = evaluations
      .filter(r => r.status === 'fulfilled')
      .map(r => (r as PromiseFulfilledResult<Evaluation>).value);
    
    if (successful.length === 0) {
      throw new Error('ëª¨ë“  LLM í‰ê°€ ì‹¤íŒ¨');
    }
    
    // êµì°¨ ê²€ì¦ ë° í†µí•©
    return this.consolidateResults(successful);
  }
  
  private consolidateResults(evaluations: Evaluation[]): ConsolidatedResult {
    const scores = {
      ì—…ë¬´ëŠ¥ë ¥: this.calculateScore(evaluations, 'ì—…ë¬´ëŠ¥ë ¥'),
      ë¬¸ì¥ë ¥: this.calculateScore(evaluations, 'ë¬¸ì¥ë ¥'),
      ê¸°ë³¸íƒœë„: this.calculateScore(evaluations, 'ê¸°ë³¸íƒœë„')
    };
    
    const consistency = this.calculateConsistency(evaluations);
    const confidence = this.calculateConfidence(evaluations);
    
    return {
      scores,
      totalScore: this.calculateTotalScore(scores),
      consistency,
      confidence,
      providers: evaluations.map(e => ({
        name: e.provider,
        scores: e.scores,
        responseTime: e.responseTime
      })),
      evidence: this.mergeEvidence(evaluations)
    };
  }
  
  private calculateScore(evaluations: Evaluation[], category: string): number {
    const scores = evaluations.map(e => e.scores[category]);
    
    // ì´ìƒì¹˜ ì œê±° í›„ í‰ê· 
    const filtered = this.removeOutliers(scores);
    return filtered.reduce((a, b) => a + b, 0) / filtered.length;
  }
  
  private removeOutliers(values: number[]): number[] {
    const sorted = [...values].sort((a, b) => a - b);
    const q1 = sorted[Math.floor(sorted.length * 0.25)];
    const q3 = sorted[Math.floor(sorted.length * 0.75)];
    const iqr = q3 - q1;
    
    return values.filter(v => 
      v >= q1 - 1.5 * iqr && v <= q3 + 1.5 * iqr
    );
  }
}
```

### 2.4 ì„¤ì • ê¸°ë°˜ í‰ê°€ ê¸°ì¤€ ê´€ë¦¬

#### ì™¸ë¶€ ì„¤ì • íŒŒì¼
```json
// config/evaluation/criteria-v4.3.json
{
  "version": "4.3",
  "lastUpdated": "2024-01-15",
  "author": "CX Team",
  "criteria": {
    "ì—…ë¬´ëŠ¥ë ¥": {
      "weight": 0.6,
      "subcriteria": {
        "ê³ ê°_ì§ˆë¬¸_ë‚´ìš©_íŒŒì•…": {
          "weight": 0.167,
          "description": "ê³ ê°ì˜ ì§ˆë¬¸ê³¼ ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ íŒŒì•…",
          "evaluation_points": [
            "ì§ˆë¬¸ì˜ í•µì‹¬ íŒŒì•… ì—¬ë¶€",
            "ì¶”ê°€ ì •ë³´ ìš”ì²­ì˜ ì ì ˆì„±",
            "ë¬¸ì œ ì •ì˜ì˜ ëª…í™•ì„±"
          ]
        },
        "íŒŒì•…_ë°_í•´ê²°_ì ê·¹ì„±": {
          "weight": 0.167,
          "description": "ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì ê·¹ì  ë…¸ë ¥",
          "evaluation_points": [
            "ëŠ¥ë™ì  í•´ê²° ì‹œë„",
            "ëŒ€ì•ˆ ì œì‹œ ì—¬ë¶€",
            "follow-up ì§ˆë¬¸"
          ]
        },
        "ë‹µë³€ì˜_ì •í™•ì„±_ë°_ì í•©ì„±": {
          "weight": 0.167,
          "description": "ì œê³µí•œ ì •ë³´ì˜ ì •í™•ì„±ê³¼ ì ì ˆì„±",
          "evaluation_points": [
            "ì •ë³´ì˜ ì •í™•ì„±",
            "ìƒí™©ì— ë§ëŠ” ë‹µë³€",
            "ì˜¤í•´ì˜ ì†Œì§€ ì—†ìŒ"
          ]
        },
        "ë„ë©”ì¸_ì „ë¬¸ì„±": {
          "weight": 0.167,
          "description": "ê¸ˆìœµ ë„ë©”ì¸ ì§€ì‹ í™œìš©",
          "evaluation_points": [
            "ì „ë¬¸ ìš©ì–´ ì ì ˆ ì‚¬ìš©",
            "ì •í™•í•œ í”„ë¡œì„¸ìŠ¤ ì•ˆë‚´",
            "ê·œì • ì¤€ìˆ˜"
          ]
        },
        "ì‹ ì†í•œ_ì‘ëŒ€": {
          "weight": 0.167,
          "description": "ì‘ë‹µ ì†ë„ì™€ íš¨ìœ¨ì„±",
          "evaluation_points": [
            "ì²« ì‘ë‹µ ì‹œê°„",
            "ë¬¸ì œ í•´ê²° ì‹œê°„",
            "ë¶ˆí•„ìš”í•œ ì§€ì—° ì—†ìŒ"
          ]
        },
        "ìƒí™©_ê³µê°": {
          "weight": 0.167,
          "description": "ê³ ê° ìƒí™© ì´í•´ì™€ ê³µê°",
          "evaluation_points": [
            "ìƒí™© íŒŒì•… ì •í™•ë„",
            "í•´ê²° ë°©í–¥ ì œì‹œ",
            "ê³ ê° ì…ì¥ ê³ ë ¤"
          ]
        }
      }
    },
    "ë¬¸ì¥ë ¥": {
      "weight": 0.25,
      "subcriteria": {
        "ì •í™•í•œ_ë§ì¶¤ë²•": {
          "weight": 0.25,
          "description": "ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì˜ ì •í™•ì„±"
        },
        "ì ì ˆí•œ_ì–¸ì–´_í‘œí˜„": {
          "weight": 0.25,
          "description": "ìƒí™©ì— ë§ëŠ” ì–¸ì–´ ì‚¬ìš©"
        },
        "ì‰¬ìš´_í‘œí˜„_ì‚¬ìš©": {
          "weight": 0.25,
          "description": "ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…"
        },
        "ë‹¨ê³„ë³„_ì•ˆë‚´": {
          "weight": 0.25,
          "description": "ì²´ê³„ì ì´ê³  ìˆœì°¨ì ì¸ ì•ˆë‚´"
        }
      }
    },
    "ê¸°ë³¸_íƒœë„": {
      "weight": 0.15,
      "subcriteria": {
        "ì¸ì‚¬_ë°_ì¶”ê°€_ë¬¸ì˜": {
          "weight": 0.5,
          "description": "ì ì ˆí•œ ì¸ì‚¬ì™€ ì¶”ê°€ ë„ì›€ ì œì•ˆ"
        },
        "ì–‘í•´_í‘œí˜„_ì‚¬ìš©": {
          "weight": 0.5,
          "description": "ë¶ˆí¸ì— ëŒ€í•œ ì–‘í•´ êµ¬í•˜ê¸°"
        }
      }
    }
  },
  "scoring": {
    "scale": {
      "min": 1.0,
      "max": 5.0,
      "step": 0.1
    },
    "thresholds": {
      "excellent": 4.6,
      "good": 4.2,
      "average": 3.8,
      "poor": 3.0
    },
    "problematic_criteria": {
      "total_score": 3.8,
      "ì—…ë¬´ëŠ¥ë ¥": 3.5,
      "ë¬¸ì¥ë ¥": 3.0,
      "ê¸°ë³¸íƒœë„": 3.0,
      "relative_threshold": 0.3
    }
  },
  "filters": {
    "exclude_patterns": [
      "15ë¶„ ìë™ ë©”ì‹œì§€",
      "30ë¶„ ìë™ ë©”ì‹œì§€",
      "ì•ˆë…•í•˜ì„¸ìš” ê³ ê°ë‹˜, í•€ë‹¤ ê³ ê°ê²½í—˜íŒ€"
    ],
    "exclude_tags": [
      "ê¸°íƒ€_í…ŒìŠ¤íŠ¸",
      "í…ŒìŠ¤íŠ¸"
    ],
    "include_tags": [
      "ìë™ì¢…ë£Œ",
      "ìˆ˜ë™ì¢…ë£Œ"
    ]
  }
}
```

#### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬
```json
// config/prompts/evaluation-v4.3.json
{
  "version": "4.3",
  "templates": {
    "system": {
      "role": "system",
      "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ì„œë¹„ìŠ¤ ê³ ê° ìƒë‹´ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ ê°ê´€ì ì´ê³  ì¼ê´€ì„± ìˆëŠ” í‰ê°€ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."
    },
    "user": {
      "role": "user",
      "content": "ë‹¤ìŒ ìƒë‹´ ë‚´ìš©ì„ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•´ì£¼ì„¸ìš”.\n\ní‰ê°€ ê¸°ì¤€:\n{{criteria}}\n\nìƒë‹´ ë‚´ìš©:\n{{chat_content}}\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n{{response_format}}"
    },
    "response_format": {
      "scores": {
        "ì—…ë¬´ëŠ¥ë ¥": {
          "ê³ ê°_ì§ˆë¬¸_ë‚´ìš©_íŒŒì•…": "number",
          "íŒŒì•…_ë°_í•´ê²°_ì ê·¹ì„±": "number",
          "ë‹µë³€ì˜_ì •í™•ì„±_ë°_ì í•©ì„±": "number",
          "ë„ë©”ì¸_ì „ë¬¸ì„±": "number",
          "ì‹ ì†í•œ_ì‘ëŒ€": "number",
          "ìƒí™©_ê³µê°": "number",
          "subtotal": "number"
        },
        "ë¬¸ì¥ë ¥": {
          "ì •í™•í•œ_ë§ì¶¤ë²•": "number",
          "ì ì ˆí•œ_ì–¸ì–´_í‘œí˜„": "number",
          "ì‰¬ìš´_í‘œí˜„_ì‚¬ìš©": "number",
          "ë‹¨ê³„ë³„_ì•ˆë‚´": "number",
          "subtotal": "number"
        },
        "ê¸°ë³¸_íƒœë„": {
          "ì¸ì‚¬_ë°_ì¶”ê°€_ë¬¸ì˜": "number",
          "ì–‘í•´_í‘œí˜„_ì‚¬ìš©": "number",
          "subtotal": "number"
        },
        "total_score": "number"
      },
      "evidence": {
        "positive": ["string"],
        "negative": ["string"],
        "quotes": ["string"]
      },
      "problematic": "boolean",
      "severity": "high|medium|low|none"
    }
  }
}
```

---

## ğŸ“‹ 3. êµ¬ì²´ì  êµ¬í˜„ ê³„íš

### 3.1 Phase 1: ê¸°ë°˜ êµ¬ì¡° ê°œì„  (1ì£¼ì°¨)

#### Task 1.1: LLM Provider ì¶”ìƒí™” êµ¬í˜„
**íŒŒì¼ ìƒì„±/ìˆ˜ì •:**
```typescript
// lib/llm/providers/base.ts (ì‹ ê·œ)
export interface LLMProvider {
  name: string;
  model: string;
  evaluate(prompt: string): Promise<EvaluationResult>;
  getCost(tokens: number): number;
}

// lib/llm/providers/openai.ts (ì‹ ê·œ)
export class OpenAIProvider implements LLMProvider {
  // êµ¬í˜„ ë‚´ìš©
}

// lib/llm/providers/gemini.ts (ì‹ ê·œ)
export class GeminiProvider implements LLMProvider {
  name = 'gemini';
  model = process.env.GEMINI_MODEL || 'gemini-pro';
  
  async evaluate(prompt: string): Promise<EvaluationResult> {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent?key=${process.env.GEMINI_API_KEY}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: prompt }]
          }],
          generationConfig: {
            temperature: 0.1,
            topK: 1,
            topP: 0.95,
            maxOutputTokens: 2048,
          }
        })
      }
    );
    
    if (!response.ok) {
      throw new APIError(`Gemini API error: ${response.status}`);
    }
    
    return this.parseResponse(await response.json());
  }
  
  getCost(tokens: number): number {
    return (tokens / 1000) * 0.00025; // Gemini Pro pricing
  }
}
```

#### Task 1.2: ì„¤ì • íŒŒì¼ ì™¸ë¶€í™”
**íŒŒì¼ ìƒì„±:**
- `config/evaluation/criteria-v4.3.json`
- `config/prompts/evaluation-v4.3.json`
- `config/models.json`

**í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€ (.env.local):**
```bash
# LLM API Keys
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
CLAUDE_API_KEY=... # Optional

# Model Configuration
OPENAI_MODEL=gpt-4-turbo-preview
GEMINI_MODEL=gemini-pro
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000

# Evaluation Settings
EVALUATION_VERSION=4.3
ENABLE_MULTI_LLM=true
MIN_CONSENSUS_PROVIDERS=2
```

#### Task 1.3: ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
**íŒŒì¼ ìƒì„±:**
```typescript
// lib/utils/retry.ts (ì‹ ê·œ)
export async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries = 3,
  backoffMs = 1000
): Promise<T> {
  let lastError: Error;
  
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;
      
      if (i < maxRetries - 1) {
        const delay = backoffMs * Math.pow(2, i);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }
  
  throw new Error(`Failed after ${maxRetries} retries: ${lastError.message}`);
}

// lib/errors/api-error.ts (ì‹ ê·œ)
export class APIError extends Error {
  constructor(
    message: string,
    public statusCode?: number,
    public provider?: string
  ) {
    super(message);
    this.name = 'APIError';
  }
}
```

### 3.2 Phase 2: Multi-LLM í†µí•© (2ì£¼ì°¨)

#### Task 2.1: Cross Validator êµ¬í˜„
**íŒŒì¼ ìƒì„±:**
```typescript
// lib/evaluation/cross-validator.ts (ì‹ ê·œ)
import { OpenAIProvider } from '@/lib/llm/providers/openai';
import { GeminiProvider } from '@/lib/llm/providers/gemini';

export class CrossValidator {
  private providers: LLMProvider[];
  
  constructor() {
    this.providers = this.initializeProviders();
  }
  
  private initializeProviders(): LLMProvider[] {
    const providers: LLMProvider[] = [];
    
    if (process.env.OPENAI_API_KEY) {
      providers.push(new OpenAIProvider());
    }
    
    if (process.env.GEMINI_API_KEY) {
      providers.push(new GeminiProvider());
    }
    
    if (providers.length < 2) {
      console.warn('Multi-LLM ëª¨ë“œë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ Providerê°€ í•„ìš”í•©ë‹ˆë‹¤');
    }
    
    return providers;
  }
  
  // í‰ê°€ ë©”ì„œë“œ êµ¬í˜„
}
```

#### Task 2.2: API Route ìˆ˜ì •
**íŒŒì¼ ìˆ˜ì •:**
```typescript
// app/api/analyze-multi/route.ts (ì‹ ê·œ)
import { NextRequest } from 'next/server';
import { CrossValidator } from '@/lib/evaluation/cross-validator';
import { PromptGenerator } from '@/lib/prompt/generator';

export async function POST(request: NextRequest) {
  try {
    const { chatData, useMultiLLM = true } = await request.json();
    
    if (!chatData) {
      return Response.json({ error: 'ìƒë‹´ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤' }, { status: 400 });
    }
    
    const validator = new CrossValidator();
    const promptGen = new PromptGenerator();
    
    // í”„ë¡¬í”„íŠ¸ ìƒì„±
    const prompt = promptGen.generatePrompt(chatData);
    
    // Multi-LLM í‰ê°€
    const result = useMultiLLM 
      ? await validator.evaluateWithMultiple(prompt)
      : await validator.evaluateWithSingle(prompt);
    
    return Response.json({
      success: true,
      evaluation: result,
      metadata: {
        version: process.env.EVALUATION_VERSION,
        providers: result.providers,
        consistency: result.consistency,
        timestamp: new Date().toISOString()
      }
    });
    
  } catch (error) {
    console.error('í‰ê°€ ì˜¤ë¥˜:', error);
    return Response.json(
      { error: 'í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤', details: error.message },
      { status: 500 }
    );
  }
}
```

#### Task 2.3: ê¸°ì¡´ Route ë§ˆì´ê·¸ë ˆì´ì…˜
**íŒŒì¼ ìˆ˜ì • ëª©ë¡:**
- `app/api/analyze-individual/route.ts`
- `app/api/analyze-comprehensive/route.ts`
- `app/api/analyze-individual-new/route.ts`
- `app/api/analyze-counselor-comprehensive/route.ts`

**ìˆ˜ì • ë‚´ìš©:**
```typescript
// app/api/analyze-individual/route.ts (ìˆ˜ì •)
import { CrossValidator } from '@/lib/evaluation/cross-validator';

// AS-IS
async function callOpenAI(prompt: string, apiKey: string) {
  // ê¸°ì¡´ ì½”ë“œ
}

// TO-BE
async function evaluateChat(chatData: any) {
  const validator = new CrossValidator();
  const result = await validator.evaluate(chatData);
  return result;
}

export async function POST(request: NextRequest) {
  try {
    const data = await request.json();
    
    // ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë©´ì„œ í‰ê°€ ë¶€ë¶„ë§Œ êµì²´
    const evaluation = await evaluateChat(data.chatData);
    
    return Response.json({
      success: true,
      evaluation,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    // ì—ëŸ¬ ì²˜ë¦¬
  }
}
```

### 3.3 Phase 3: ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” (3ì£¼ì°¨)

#### Task 3.1: ë¹„ìš© ì¶”ì  ì‹œìŠ¤í…œ
**íŒŒì¼ ìƒì„±:**
```typescript
// lib/tracking/cost-tracker.ts (ì‹ ê·œ)
export class CostTracker {
  private static instance: CostTracker;
  private costs: Map<string, number> = new Map();
  
  static getInstance(): CostTracker {
    if (!this.instance) {
      this.instance = new CostTracker();
    }
    return this.instance;
  }
  
  track(provider: string, model: string, tokens: number, cost: number) {
    const key = `${provider}:${model}`;
    const current = this.costs.get(key) || 0;
    this.costs.set(key, current + cost);
    
    // ì¼ë³„ ì§‘ê³„ë¥¼ ìœ„í•œ ë¡œê¹…
    console.log(`[Cost] ${provider} ${model}: ${tokens} tokens = $${cost.toFixed(4)}`);
  }
  
  getDailySummary(): CostSummary {
    const summary: CostSummary = {
      total: 0,
      byProvider: {},
      timestamp: new Date().toISOString()
    };
    
    this.costs.forEach((cost, key) => {
      const [provider] = key.split(':');
      summary.byProvider[provider] = (summary.byProvider[provider] || 0) + cost;
      summary.total += cost;
    });
    
    return summary;
  }
  
  reset() {
    this.costs.clear();
  }
}
```

#### Task 3.2: í‰ê°€ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§
**íŒŒì¼ ìƒì„±:**
```typescript
// components/monitoring/ConsistencyMonitor.tsx (ì‹ ê·œ)
import { useEffect, useState } from 'react';

export function ConsistencyMonitor() {
  const [stats, setStats] = useState<ConsistencyStats | null>(null);
  
  useEffect(() => {
    fetchStats();
    const interval = setInterval(fetchStats, 60000); // 1ë¶„ë§ˆë‹¤ ê°±ì‹ 
    return () => clearInterval(interval);
  }, []);
  
  const fetchStats = async () => {
    const response = await fetch('/api/stats/consistency');
    const data = await response.json();
    setStats(data);
  };
  
  if (!stats) return <div>Loading...</div>;
  
  return (
    <Card>
      <CardHeader>
        <CardTitle>í‰ê°€ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§</CardTitle>
      </CardHeader>
      <CardContent>
        <div className="space-y-4">
          <div>
            <Label>ì „ì²´ ì¼ê´€ì„±</Label>
            <Progress value={stats.overall * 100} />
            <span className="text-sm text-muted-foreground">
              {(stats.overall * 100).toFixed(1)}%
            </span>
          </div>
          
          <div className="grid grid-cols-3 gap-4">
            <div>
              <Label>ì—…ë¬´ëŠ¥ë ¥</Label>
              <div className="text-2xl font-bold">
                {(stats.byCategory.ì—…ë¬´ëŠ¥ë ¥ * 100).toFixed(1)}%
              </div>
            </div>
            <div>
              <Label>ë¬¸ì¥ë ¥</Label>
              <div className="text-2xl font-bold">
                {(stats.byCategory.ë¬¸ì¥ë ¥ * 100).toFixed(1)}%
              </div>
            </div>
            <div>
              <Label>ê¸°ë³¸íƒœë„</Label>
              <div className="text-2xl font-bold">
                {(stats.byCategory.ê¸°ë³¸íƒœë„ * 100).toFixed(1)}%
              </div>
            </div>
          </div>
          
          <div>
            <Label>Providerë³„ ì‘ë‹µ ì‹œê°„</Label>
            {stats.providers.map(p => (
              <div key={p.name} className="flex justify-between">
                <span>{p.name}</span>
                <span>{p.avgResponseTime}ms</span>
              </div>
            ))}
          </div>
        </div>
      </CardContent>
    </Card>
  );
}
```

### 3.4 Phase 4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (4ì£¼ì°¨)

#### Task 4.1: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
**íŒŒì¼ ìƒì„±:**
```typescript
// __tests__/lib/llm/providers/openai.test.ts (ì‹ ê·œ)
import { OpenAIProvider } from '@/lib/llm/providers/openai';

describe('OpenAIProvider', () => {
  let provider: OpenAIProvider;
  
  beforeEach(() => {
    provider = new OpenAIProvider();
  });
  
  test('should evaluate chat successfully', async () => {
    const prompt = 'Test prompt';
    const result = await provider.evaluate(prompt);
    
    expect(result).toHaveProperty('scores');
    expect(result.scores).toHaveProperty('ì—…ë¬´ëŠ¥ë ¥');
    expect(result.scores.ì—…ë¬´ëŠ¥ë ¥).toBeGreaterThanOrEqual(1);
    expect(result.scores.ì—…ë¬´ëŠ¥ë ¥).toBeLessThanOrEqual(5);
  });
  
  test('should calculate cost correctly', () => {
    const tokens = 1000;
    const cost = provider.getCost(tokens);
    
    expect(cost).toBeGreaterThan(0);
    expect(cost).toBeLessThan(1); // 1ë‹¬ëŸ¬ ë¯¸ë§Œ
  });
});

// __tests__/lib/evaluation/cross-validator.test.ts (ì‹ ê·œ)
import { CrossValidator } from '@/lib/evaluation/cross-validator';

describe('CrossValidator', () => {
  let validator: CrossValidator;
  
  beforeEach(() => {
    validator = new CrossValidator();
  });
  
  test('should consolidate multiple evaluations', async () => {
    const chatData = { /* test data */ };
    const result = await validator.evaluate(chatData);
    
    expect(result).toHaveProperty('scores');
    expect(result).toHaveProperty('consistency');
    expect(result.consistency).toBeGreaterThanOrEqual(0);
    expect(result.consistency).toBeLessThanOrEqual(1);
  });
  
  test('should handle provider failures gracefully', async () => {
    // í•œ providerê°€ ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ë¡œ í‰ê°€ ì§„í–‰
    const result = await validator.evaluate(chatData);
    
    expect(result.providers.length).toBeGreaterThanOrEqual(1);
  });
});
```

#### Task 4.2: í†µí•© í…ŒìŠ¤íŠ¸
**íŒŒì¼ ìƒì„±:**
```typescript
// __tests__/api/analyze-multi.test.ts (ì‹ ê·œ)
import { POST } from '@/app/api/analyze-multi/route';

describe('Multi-LLM Analysis API', () => {
  test('should return consistent evaluation', async () => {
    const request = new Request('http://localhost:3000/api/analyze-multi', {
      method: 'POST',
      body: JSON.stringify({
        chatData: { /* test chat data */ },
        useMultiLLM: true
      })
    });
    
    const response = await POST(request);
    const data = await response.json();
    
    expect(response.status).toBe(200);
    expect(data.success).toBe(true);
    expect(data.evaluation).toHaveProperty('consistency');
    expect(data.evaluation.consistency).toBeGreaterThan(0.8);
  });
});
```

---

## ğŸ“Š 4. ì˜ˆìƒ ê²°ê³¼ ë° KPI

### 4.1 í•µì‹¬ ì„±ê³¼ ì§€í‘œ

| KPI | AS-IS | TO-BE | ê°œì„ ìœ¨ |
|-----|-------|-------|--------|
| **í‰ê°€ ì¼ê´€ì„±** | 85% | 95% | +11.8% |
| **í‰ê°€ ì‹ ë¢°ë„** | 70% | 90% | +28.6% |
| **API ì‘ë‹µ ì‹œê°„** | 3ì´ˆ | 2ì´ˆ | -33.3% |
| **ì›”ê°„ ë¹„ìš©** | $50 | $35 | -30% |
| **ì¬í‰ê°€ ìš”ì²­** | 15% | 5% | -66.7% |
| **ìš´ì˜ íš¨ìœ¨ì„±** | - | +40% | - |

### 4.2 ë¹„ìš© ë¶„ì„

#### ì›”ê°„ ì˜ˆìƒ ë¹„ìš© (10,000ê±´ ê¸°ì¤€)
```
AS-IS (OpenAIë§Œ ì‚¬ìš©):
- GPT-4: $0.01/1K tokens Ã— 2K tokens Ã— 10,000 = $200

TO-BE (Multi-LLM):
- OpenAI GPT-4: $0.01 Ã— 1K Ã— 10,000 = $100
- Gemini Pro: $0.00025 Ã— 1K Ã— 10,000 = $2.50
- ì´í•©: $102.50 (48.75% ì ˆê°)
```

### 4.3 ROI ê³„ì‚°
```
íˆ¬ì ë¹„ìš©:
- ê°œë°œ ì¸ê±´ë¹„: 1ëª… Ã— 4ì£¼ = 1,000ë§Œì›
- ì¸í”„ë¼ ë¹„ìš©: 0ì› (ê¸°ì¡´ í™œìš©)
- ì´ íˆ¬ì: 1,000ë§Œì›

ì›”ê°„ ì´ìµ:
- API ë¹„ìš© ì ˆê°: $97.50 (ì•½ 13ë§Œì›)
- ìš´ì˜ ì‹œê°„ ì ˆê°: 40ì‹œê°„ (ì•½ 200ë§Œì›)
- ì›”ê°„ ì´ ì´ìµ: 213ë§Œì›

ROI:
- íˆ¬ì íšŒìˆ˜ ê¸°ê°„: 4.7ê°œì›”
- 1ë…„ ROI: 155%
```

---

## ğŸ“… 5. êµ¬í˜„ ì¼ì •

### Week 1 (1/15 - 1/21)
- [x] LLM Provider ì¶”ìƒí™”
- [x] ì„¤ì • íŒŒì¼ ì™¸ë¶€í™”
- [x] ì—ëŸ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜

### Week 2 (1/22 - 1/28)
- [ ] Cross Validator êµ¬í˜„
- [ ] Multi-LLM API ê°œë°œ
- [ ] ê¸°ì¡´ Route ë§ˆì´ê·¸ë ˆì´ì…˜

### Week 3 (1/29 - 2/4)
- [ ] ë¹„ìš© ì¶”ì  ì‹œìŠ¤í…œ
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- [ ] ì„±ëŠ¥ ìµœì í™”

### Week 4 (2/5 - 2/11)
- [ ] í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ë¬¸ì„œí™”
- [ ] ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### Week 5 (2/12 - 2/15)
- [ ] ì•ˆì •í™”
- [ ] í”¼ë“œë°± ìˆ˜ì§‘
- [ ] ìµœì¢… ì¡°ì •

---

## ğŸš¨ 6. ë¦¬ìŠ¤í¬ ê´€ë¦¬

### 6.1 ê¸°ìˆ ì  ë¦¬ìŠ¤í¬

| ë¦¬ìŠ¤í¬ | ë°œìƒ ê°€ëŠ¥ì„± | ì˜í–¥ë„ | ëŒ€ì‘ ë°©ì•ˆ |
|--------|------------|--------|-----------|
| LLM API ì¥ì•  | ì¤‘ | ë†’ìŒ | ë‹¤ì¤‘ Providerë¡œ Failover |
| ì¼ê´€ì„± ì €í•˜ | ë‚®ìŒ | ë†’ìŒ | êµì°¨ ê²€ì¦ ê°•í™” |
| ë¹„ìš© ì´ˆê³¼ | ë‚®ìŒ | ì¤‘ | ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§ |
| ì„±ëŠ¥ ì €í•˜ | ì¤‘ | ì¤‘ | ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” |

### 6.2 ìš´ì˜ ë¦¬ìŠ¤í¬

| ë¦¬ìŠ¤í¬ | ëŒ€ì‘ ë°©ì•ˆ |
|--------|-----------|
| í‰ê°€ ê¸°ì¤€ ë³€ê²½ | JSON ì„¤ì •ìœ¼ë¡œ ì¦‰ì‹œ ë°˜ì˜ |
| Provider ì¶”ê°€/ì œê±° | í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°ë¡œ ìœ ì—° ëŒ€ì‘ |
| ë°ì´í„° ë³´ì•ˆ | API Key í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ |

---

## âœ… 7. ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ ì „ ì¤€ë¹„
- [ ] API Key ë°œê¸‰ (OpenAI, Gemini)
- [ ] ê°œë°œ í™˜ê²½ ì„¤ì •
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

### ê°œë°œ ì¤‘
- [ ] ì½”ë“œ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰

### ë°°í¬ ì „
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ë³´ì•ˆ ê²€í† 
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

### ë°°í¬ í›„
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ì•Œë¦¼ êµ¬ì„±
- [ ] ë¡¤ë°± ê³„íš ìˆ˜ë¦½

---

## ğŸ“š 8. ì°¸ê³  ìë£Œ

### API ë¬¸ì„œ
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Google Gemini API](https://ai.google.dev/api/rest)
- [Anthropic Claude API](https://docs.anthropic.com/claude/reference)

### ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
- [Zod - TypeScript ìŠ¤í‚¤ë§ˆ ê²€ì¦](https://zod.dev/)
- [React Query - ì„œë²„ ìƒíƒœ ê´€ë¦¬](https://tanstack.com/query)

### ë‚´ë¶€ ë¬¸ì„œ
- í‰ê°€ ê¸°ì¤€ ê°€ì´ë“œë¼ì¸ v4.2
- CXíŒ€ ìƒë‹´ í’ˆì§ˆ ê´€ë¦¬ ë§¤ë‰´ì–¼

---

## ğŸ“ 9. ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ì‘ì„±ì | ë³€ê²½ ë‚´ìš© |
|------|------|--------|-----------|
| 1.0 | 2024-01-15 | CX Team | ì´ˆì•ˆ ì‘ì„± |
| 1.1 | 2024-01-16 | Tech Team | ê¸°ìˆ  ê²€í†  ë° ìˆ˜ì • |

---

**ë¬¸ì„œ ìŠ¹ì¸**

- ê¸°íš: CX Team Lead
- ê°œë°œ: Tech Lead
- ê²€í† : Product Manager

*ë³¸ ë¬¸ì„œëŠ” í•€ë‹¤ CXíŒ€ ìƒë‹´ í‰ê°€ ì‹œìŠ¤í…œ ê°œì„  í”„ë¡œì íŠ¸ì˜ ê³µì‹ ê¸°íšì„œì…ë‹ˆë‹¤.*